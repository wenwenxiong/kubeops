---
  - name: verify hostname
    fail: msg="provided hostname does not match reported hostname of {{ ansible_nodename }}"
    failed_when: "ansible_nodename not in [ inventory_hostname, inventory_hostname_short ]"

  - name: verify systemd
    fail: msg="systemd is required"
    failed_when: ansible_service_mgr != "systemd"

  # kubernetes checks /proc/swaps lines > 1
  # don't verify if host has only etcd role
  - name: list memory swaps in /proc/swaps
    command: cat /proc/swaps
    register: memory_swaps
    failed_when: false
    when: >
      not ((kubelet_overrides is defined and 
      kubelet_overrides['fail-swap-on'] is defined and 
      kubelet_overrides['fail-swap-on'] == 'false') or 
      (kubelet_node_overrides[inventory_hostname] is defined and 
      kubelet_node_overrides[inventory_hostname]['fail-swap-on'] is defined and 
      kubelet_node_overrides[inventory_hostname]['fail-swap-on'] == 'false')) and 
      ('etcd' not in group_names or
      ('etcd' in group_names and (group_names | length > 1)))
      

  - name: fail if memory swap is not disabled
    fail:
      msg: "Memory swap is enabled on the node, disable it or set '--fail-swap-on=false' on the kubelet"
    when: >
      memory_swaps is defined and 
      memory_swaps.rc is defined and 
      (memory_swaps.rc != 0 or 
      (memory_swaps.stdout_lines is defined and memory_swaps.stdout_lines|length > 1)) and
      ('etcd' not in group_names or
      ('etcd' in group_names and (group_names | length > 1)))

  - name: validate devicemapper direct-lvm block device
    include: direct_lvm_preflight.yaml
    when: >
      ansible_os_family == 'RedHat' and 
      docker.storage.driver == 'devicemapper' and 
      docker.storage.direct_lvm_block_device.path != ''

  # Every etcd node should be able to reach all etcd nodes. This is quadratic,
  # but we can live with it because etcd count is usually <= 5
  - name: verify etcd to etcd node connectivity using IP
    command: ping -c 2 {{ item }}
    # Using map here to get the right item shown in stdout
    with_items: "{{ groups['etcd']|map('extract', hostvars, 'internal_ipv4')|list }}"
    when: "'etcd' in group_names"
  - name: verify etcd to etcd node connectivity using hostname
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['etcd'] }}"
    when: "'etcd' in group_names"

  # Every master node should be able to reach all etcd nodes
  - name: verify master node to etcd node connectivity using IP
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['etcd']|map('extract', hostvars, 'internal_ipv4')|list }}"
    when: "'master' in group_names"
  - name: verify master node to etcd node connectivity using hostname
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['etcd'] }}"
    when: "'master' in group_names"

  # Every worker node should be able to reach all master nodes
  - name: verify worker node to master node connectivity using IP
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['master']|map('extract', hostvars, 'internal_ipv4')|list }}"
    when: "'worker' in group_names"
  - name: verify worker node to master node connectivity using hostname
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['master'] }}"
    when: "'worker' in group_names"

  # Every ingress node should be able to reach all master nodes
  - name: verify ingress node to master node connectivity using IP
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['master']|map('extract', hostvars, 'internal_ipv4')|list }}"
    when: "'ingress' in group_names"
  - name: verify ingress node to master node connectivity using hostname
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['master'] }}"
    when: "'ingress' in group_names"

  # Every ingress node should be able to reach all worker nodes
  - name: verify ingress node to worker node connectivity using IP
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['worker']|map('extract', hostvars, 'internal_ipv4')|list }}"
    when: "'ingress' in group_names"
  - name: verify ingress node to worker node connectivity using hostname
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['worker'] }}"
    when: "'ingress' in group_names"

  # Every worker node should be able to reach all worker nodes.
  # We use a random sampling of worker nodes to avoid quadratic complexity.
  - name: verify worker to worker node connectivity with random sample
    include: random_ping.yaml
    with_items: # Ping three nodes at random
      - 1
      - 2
      - 3
    loop_control:
      loop_var: outer_item # Define this (even thought we don't use it) so that ansible doesn't complain.
    when: "'worker' in group_names"

  # Run from the install node, 
  # Check if the helm repos can be reached
  - name: verify install node can reach official helm chart repo
    uri:
      url: https://kubernetes-charts.storage.googleapis.com
    delegate_to: 127.0.0.1
    become: no
    run_once: true
    when: helm.enabled|bool == true and disconnected_installation|bool != true

  # setup Kismatic Inspector
  - name: copy Kismatic Inspector to node
    copy:
      src: "{{ kismatic_preflight_checker }}"
      dest: "{{ bin_dir }}/kismatic-inspector"
      mode: 0744

  - name: copy kismatic-inspector.service to remote
    template:
      src: kismatic-inspector.service.j2
      dest: "{{ init_system_dir }}/kismatic-inspector.service"
    notify:
      - reload services

  - meta: flush_handlers  #Run handlers

  - name: start kismatic-inspector service
    service:
      name: kismatic-inspector.service
      state: restarted # always restart to ensure that any existing inspectors are replaced by this one

  # Run the pre-flights checks, and always stop the checker regardless of result
  - block:
      - name: run pre-flight checks using Kismatic Inspector from the master
        command: '{{ bin_dir }}/kismatic-inspector client {{ internal_ipv4 }}:8888 -o json --node-roles {{ ",".join(group_names) }} {% if upgrading|default("false")|bool %}--upgrade{% endif %} --additional-vars kubernetes_yum_version={{ kubernetes_yum_version }},kubernetes_deb_version={{ kubernetes_deb_version }}'
        delegate_to: "{{ groups['master'][0] }}"
        register: out
      - name: run pre-flight checks using Kismatic Inspector from the worker
        command: '{{ bin_dir }}/kismatic-inspector client {{ internal_ipv4 }}:8888 -o json --node-roles {{ ",".join(group_names) }} {% if upgrading|default("false")|bool %}--upgrade{% endif %} --additional-vars kubernetes_yum_version={{ kubernetes_yum_version }},kubernetes_deb_version={{ kubernetes_deb_version }}'
        delegate_to: "{{ groups['worker'][0] }}"
        register: out
    always:
      - name: stop kismatic-inspector service
        service:
          name: kismatic-inspector.service
          state: stopped
      - name: verify Kismatic Inspector succeeded
        command: /bin/true
        failed_when: "out.rc != 0"